{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Webscraping with XML Data Querying\n",
    "\n",
    "#### Christian Rivera\n",
    "- Github: https://github.com/crivera2013\n",
    "\n",
    "**The purpose of this lesson is to teach how to use python for webscraping files and texts from the internet.  This has three main uses:**\n",
    "1. **Automating previously manual tasks.**\n",
    "2. **Parsing large amounts of parsing semi-structured data.**\n",
    "3. **Getting used to data stored in Hash Table/Dictionary/JSON/NOSQL format: (each \"row\" in a dataset has a dynamic set of column attributes that can be independent from all the other \"rows\" in the set.**\n",
    "\n",
    "For this lesson, we will be using the **Requests, Pandas, **and** BeautifulSoup** libraries\n",
    "\n",
    "![alt text](https://www.explainxkcd.com/wiki/images/1/1a/api.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to HTML for Webscraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the packages\n",
    "\n",
    "**Requests** allows python to retrieve information from websites and other computers using HTTP calls.  We saw this before with API's however, if the website is not an API, **requests.get** returns the full html file of the webpage.\n",
    "\n",
    "**BeautifulSoup** is a python package that enables us to search through the HTML file for the information we want.\n",
    "\n",
    "\n",
    "[**Requests/BeautifulSoup Tutorial**](https://www.dataquest.io/blog/web-scraping-tutorial-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <head>\n",
    "        <title>Beginner Beautiful Soup HTML Example </title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h2> Double Click on this box to see html code!</h2>\n",
    "        <p class=\"bold-paragraph\">\n",
    "            Double Click This Box!\n",
    "            <a href=\"https://www.dataquest.io/blog/web-scraping-tutorial-python/\" id=\"learn-link\">Link to tutorial resource\n",
    "            </a>\n",
    "        </p>\n",
    "        <p class=\"bold-paragraph extra-large\">\n",
    "            Seriously, Double Click The Box!\n",
    "            <a href=\"http://www.smbc-comics.com/comic/2013-02-01\" class=\"extra-large\">I'm an 'a' tag and I create links\n",
    "            </a>\n",
    "        </p>\n",
    "        <div id=\"my-first-div\">\n",
    "            <img src=\"https://www.explainxkcd.com/wiki/images/1/1a/api.png\" width=150px>\n",
    "            <p id='get-me' style=\"text-align: center;\"> I'm a nested p tag with a centered style attribute nested within a div tag\n",
    "            </p>\n",
    "        </div>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### your string can span multiple lines if it starts and ends with triple apostrophes (\"\"\")\n",
    "sample_html=\"\"\"<html>\n",
    "    <head>\n",
    "        <title>Beginner Beautiful Soup HTML Example </title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h2> Double Click on this box to see html code!</h2>\n",
    "        <p class=\"bold-paragraph\">\n",
    "            Double Click This Box!\n",
    "            <a href=\"https://www.dataquest.io/blog/web-scraping-tutorial-python/\" id=\"learn-link\">Link to tutorial resource\n",
    "            </a>\n",
    "        </p>\n",
    "        <p class=\"bold-paragraph extra-large\">\n",
    "            Seriously, Double Click The Box!\n",
    "            <a href=\"http://www.smbc-comics.com/comic/2013-02-01\" class=\"extra-large\">I'm an 'a' tag and I create links\n",
    "            </a>\n",
    "        </p>\n",
    "        <div id=\"my-first-div\">\n",
    "            <img src=\"https://cdn-images-1.medium.com/max/1200/1*Qi2ta02wgA4-otNFDaFrRw.png\" width=150px>\n",
    "            <p id='get-me' style=\"text-align: center;\"> I'm a nested p tag with a centered style attribute nested within a div tag\n",
    "            </p>\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instead of using Requests to pull API data, you would have it pull regular HTML data as a string.  Afterwards you convert that string into XML format so that you can access data similar to how you would access data from your dictionaries like dog1 and dog2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML files consist of tags, nested tags and attributes/text/data stored in each of those tags.  We've seen something similar before with the python dictionaries / JSON objects attribute-value pairs.  Our sample HTML above has this structure:\n",
    "\n",
    "<div style=\"font-weight:bold'\">\n",
    "<ul>\n",
    "  <li>head\n",
    "    <ul>\n",
    "      <li>title</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>body\n",
    "  \t<ul>\n",
    "      <li>h2</li>\n",
    "      <li>p\n",
    "      \t<ul>\n",
    "        \t<li>a</li>\n",
    "      \t</ul>\n",
    "      </li>\n",
    "      <li>p\n",
    "      \t<ul>\n",
    "        \t<li>a</li>\n",
    "      \t</ul>\n",
    "      </li>\n",
    "      <li>div\n",
    "      \t<ul>\n",
    "        \t<li>img</li>\n",
    "            <li>p</li>\n",
    "      \t</ul>\n",
    "      </li>\n",
    "    </ul>\n",
    "</ul>\n",
    "</div>               \n",
    "[List of different types of HTML tags and their uses](https://developer.mozilla.org/en-US/docs/Web/HTML/Element)\n",
    "### Now let's use BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert string into XML format\n",
    "\n",
    "soup = BeautifulSoup(sample_html,'lxml')\n",
    "print(type(soup))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Meta-data Title of the sample webpage.\n",
    "\n",
    "BeautifulSoup uses dot notation when handling nested attributes and tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(soup.head.title)\n",
    "print(\"\\n\")\n",
    "print(soup.head.title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how about the body?\n",
    "soup.body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But what if we want data that is nested deep within an html file?\n",
    "\n",
    "- **soup.find( tag_name, { attribute-key-values-pairs } )**\n",
    "\n",
    "- **soup.find_all( tag_name, { attribute-key-values-pairs } )**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the first p tag in the script\n",
    "p_tag = soup.find('p')\n",
    "print(p_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return the text of that p tag\n",
    "print(p_tag.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find all the p_tags and put them in a list\n",
    "p_tags = soup.find_all('p')\n",
    "print(\"# of p tags: %s\" %len(p_tags))\n",
    "print(p_tags[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the p tag with the class: \"bold-paragraph extra-large\"\n",
    "soup.find('p',{'class':\"bold-paragraph extra-large\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the a tag with the id: \"learn-link\"\n",
    "\n",
    "a_tag = soup.find('a',{'id':'learn-link'})\n",
    "a_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract the url from that a_tag\n",
    "\n",
    "#atttributes of a tag are denoted with dictionary notation\n",
    "\n",
    "a_tag['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With the basic of BeautifulSoup down, let's do some scraping\n",
    "\n",
    "### Example 1: Basic XML file\n",
    "In your directory there is a file called \"**cd_catalog.xml**\" which we will read in and parse using BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "infile = open(\"cd_catalog.xml\",\"r\")\n",
    "contents = infile.read()\n",
    "soup = BeautifulSoup(contents,'xml')\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find all the Child tags of a given CD\n",
    "\n",
    "for child in soup.find('CD').find_all():\n",
    "    print(child.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the PRICE of the first CD listed\n",
    "\n",
    "soup.find('CD').PRICE.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extract all the artists\n",
    "for record in soup.find_all('CD'):\n",
    "    print(record.ARTIST.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example #2: Basic Webscrape\n",
    "\n",
    "**Real Example**:  Extracting transaction fee data from the Brazilian Stock Exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use requests.get to pull the HTML results \n",
    "url = \"http://www.bmfbovespa.com.br/en_us/services/fee-schedules/listed-equities-and-derivatives/equities/equities-and-investment-funds-fees/spot/\"\n",
    "\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a subsection of 1000 characters within results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response.content[2000:3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So ugly.... How do we parse this file and get the data we want?  Google Chrome and BeautifulSoup do the trick.\n",
    "\n",
    "**Copy the url above and go to the URL in your chrome browser.  Right click on the portion of the page with the data you want, and click \"inspect\" from the dropdown menu.**\n",
    "\n",
    "## Right Click on webpage ->  then click \"Inspect\" \n",
    "\n",
    "\n",
    "![alt Text](http://3qilabs.com/wp-content/uploads/2014/08/Screen-Shot-2014-08-28-at-2.20.05-PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Chrome allows you to see the location of the data you want and the tag it is associated with.\n",
    "\n",
    "Tags (shown in purple) are reference marks and an HTML file contains tags within tags within tags.  By making our way down this tree and searching for tags with specific attributes like \"class\" or \"id\", we can specify exactly the tag we are looking for and access the data found within.\n",
    "\n",
    "First we convert our **response.content** into a tree/tag format using **BeautifulSoup**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#                       #text,           #format we want to convert to.\n",
    "webpage = BeautifulSoup(response.content,'lxml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the data we want is in a table and there are table tags.  So let's find all the tags that are tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tables = webpage.find_all('table')\n",
    "print(\"Number of tables: %s\" % (len(tables)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The table we want is the second table (index 1), so we specify that.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "secondTable = tables[1]\n",
    "secondTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second table has a bunch of **td** tags that align **right** so we find all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table_values =  secondTable.find_all('td', align = 'right')\n",
    "\n",
    "print(\"Number of td tags: %s \\n\" % len(table_values))\n",
    "print(table_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 8 **td** values (2 rows with 4 columns) and we want the last one in row 1.  So that index is **3** since we start at zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "desired_value = table_values[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We extract the text from the cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "desired_value.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Finally we clean up the value to remove the percent sign and to convert into a float**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_value = desired_value.text[:-1]\n",
    "final_value = float(final_value)\n",
    "print(final_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### A trivial but still relevant example of a manual task that can be automated using python.  Let's try something more complicated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping SEC 10-K Filings\n",
    "\n",
    "There is a python package for this specific use case, but let's try it ourselves using **Requests** and **BeatifulSoup** to show how \n",
    "\n",
    "So you can search for [10-Ks manually](https://www.sec.gov/cgi-bin/browse-edgar) and access them for free.  Checking the URL for the results, we notice that after the initial url, there is a **?** question mark and afterwards a bunch of arguments and inputs including company name and type of form.  This suggests a structure for how to query data.  Let's make it work for Nvidia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://www.sec.gov/cgi-bin/browse-edgar'\n",
    "\n",
    "params = {\n",
    "    'CIK':'NVDA',\n",
    "    'action': 'getcompany',\n",
    "    'type':'10-K'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "webpage = requests.get(url, params=params)\n",
    "\n",
    "soup = BeautifulSoup(webpage.content,'lxml')\n",
    "\n",
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the HTML from Google Chrome, we can see that Table 3 has the links to **Documents**.  We want that link.  Table 3 has class name '**tableFile2**' which we can use as a search parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table =  soup.find('table', {'class':'tableFile2'})\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We want the 2nd 'tr' row (index 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row = table.find_all('tr')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cells = row.find_all('td')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's get the info we want out of those cells like 'date', 'file number' and the link to the actual document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filing_type = cells[0].text\n",
    "filing_date = cells[3].contents[0]\n",
    "file_number = cells[4].a.contents[0]\n",
    "doc_url = cells[1].a['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(filing_type)\n",
    "print(filing_date)\n",
    "print(file_number)\n",
    "print(doc_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have the url we want, we scrape that page too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://www.sec.gov'+doc_url\n",
    "\n",
    "resultsPage = requests.get(url)\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the url of the SEC 10-K document\n",
    "soup2 = BeautifulSoup(resultsPage.content,'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And we dig down into the first row of the first table to find the link in the 'Document' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row = soup2.find('table').find_all('tr')[1].find_all('td')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "description = row[1].contents[0]\n",
    "final_url = row[2].a['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(description)\n",
    "print(final_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, we procure the SEC 10-K file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://www.sec.gov'+final_url\n",
    "\n",
    "resultsPage = requests.get(url)\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We pull all of the text out of the BeautifulSoup data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tenK = BeautifulSoup(resultsPage.content,'lxml')\n",
    "all_text = tenK.text\n",
    "print(all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So we got the 10-k text we wanted.  Let's wrap that all in a function so that we can do it for whichever company we search for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def secTenKScraper(companies, uid, password):\n",
    "    \n",
    "    proxyDict = { \"http\"  : 'http://'+uid+\":\"+password+\"@http-proxy.vanguard.com:80\", \n",
    "              \"https\" : 'http://'+uid+\":\"+password+\"@http-proxy.vanguard.com:80\" }\n",
    "    \n",
    "    results = {'fails':[]}\n",
    "    \n",
    "    for company in companies:\n",
    "        try:\n",
    "            url = 'https://www.sec.gov/cgi-bin/browse-edgar'\n",
    "            params = {'CIK': company, 'action':'getcompany', 'type':'10-K'}\n",
    "            \n",
    "            webpage = requests.get(url, params=params,proxies=proxyDict)\n",
    "    \n",
    "            soup = BeautifulSoup(webpage.content,'lxml')\n",
    "            cells =  soup.find('table', {'class':'tableFile2'}).find_all('tr')[1].find_all('td')\n",
    "    \n",
    "            filing_date = cells[3].contents[0]\n",
    "            file_number = cells[4].a.contents[0]\n",
    "            doc_url = cells[1].a['href']\n",
    "        \n",
    "            url = 'https://www.sec.gov'+doc_url\n",
    "            resultsPage = requests.get(url, proxies=proxyDict)\n",
    "            soup2 = BeautifulSoup(resultsPage.content,'lxml')\n",
    "            row = soup2.find('table').find_all('tr')[1].find_all('td')\n",
    "            final_url = row[2].a['href']\n",
    "        \n",
    "            url = 'https://www.sec.gov'+final_url\n",
    "            resultsPage = requests.get(url, proxies=proxyDict)\n",
    "        \n",
    "            tenK = BeautifulSoup(resultsPage.content,'lxml')\n",
    "            all_text = tenK.text\n",
    "            \n",
    "            print(company)\n",
    "                    \n",
    "            results[company] = {\n",
    "                'file_date':filing_date,\n",
    "                'file_number':file_number,\n",
    "                'url' : url,\n",
    "                '10-K' : all_text,\n",
    "                'html': resultsPage\n",
    "                            }\n",
    "        except:\n",
    "            results['fails'].append(company)\n",
    "    \n",
    "    return results\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filings = secTenKScraper(['MSFT','FB','WMT'],uid,password)\n",
    "print(filings.keys())\n",
    "print(filings['fails'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(filings['FB']['10-K'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With this function, you can now scrape any SEC 10-K filing you want and increasing the amount of data you could use for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the NASDAQ website and retrieve the \"Today's High/Low\" value and the \"P/E Ratio\" for the stock AAPL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_url = \"https://www.nasdaq.com/symbol/\"\n",
    "\n",
    "symbol = \"AAPL\"\n",
    "\n",
    "results = requests.get(base_url + symbol, proxies=proxyDict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert to BeautifulSoup data type here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use these blocks to try out code.  \n",
    "# Add blocks as you need to: \n",
    "# \"+\" button in the top left corner\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
